{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/ggao/Documents/boeing777/boeing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q1: what's the filter? why they're relevant?\n",
    "# First tweet text:\n",
    "# \"RT @SheliaBurk: @BillWrh1970 @bfraser747 @KellyannePolls He has good relations with Muslims though, doesn't he? How could people no\\u2026 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_list = os.listdir(path)\n",
    "# Just to run a small sample first\n",
    "file_list = [file_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(file_list)):\n",
    "    with open(path + file_list[i]) as input_file:\n",
    "        temp_data = pickle.load(input_file)\n",
    "    data += temp_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Need to double check the timezone of 'timestamp_ms' and 'created_at'\n",
    "### Answer: the time zone is already specified in the 'created_at'\n",
    "\n",
    "# utc = datetime.utcnow()\n",
    "for j in range(len(data)):\n",
    "    ori_time = parser.parse(data[j]['created_at'])\n",
    "\n",
    "    # Convert the time zone to be EST\n",
    "    to_zone = tz.gettz('EST')\n",
    "    data[j]['est_time'] = datetime.strftime(ori_time.astimezone(to_zone), '%Y-%m-%d')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each tweet, get the time -- truncated to day? and the tokenized tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize and clean up the tweets\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stopwords_list = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "# regex_str = r'[a-zA-Z_]{3,}|[A-Z]{2,}'\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', flags = re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', flags = re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    # remove emojis\n",
    "    emojis_re = re.compile(\n",
    "    u\"(\\ud83d[\\ude00-\\ude4f])|\"  # emoticons\n",
    "    u\"(\\ud83c[\\udf00-\\uffff])|\"  # symbols & pictographs (1 of 2)\n",
    "    u\"(\\ud83d[\\u0000-\\uddff])|\"  # symbols & pictographs (2 of 2)\n",
    "    u\"(\\ud83d[\\ude80-\\udeff])|\"  # transport & map symbols\n",
    "    u\"(\\ud83c[\\udde0-\\uddff])\"  # flags (iOS)\n",
    "    \"+\", flags=re.UNICODE)\n",
    "    \n",
    "    tokens = emojis_re.sub(r'', s)\n",
    "    \n",
    "    # remove hash-tag and mentions \n",
    "    tokens = re.sub(r\"(?:\\#)\" + '|' + r'(?:@[\\w_]+)', '', tokens)\n",
    "    # remove URL\n",
    "    tokens = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+' + '|' + r'<[^>]+>', '', tokens)\n",
    "\n",
    "    # remove emoticons\n",
    "    emoticons_re = re.compile(emoticons_str, flags = re.VERBOSE | re.IGNORECASE)\n",
    "    tokens = emoticons_re.sub(r'', tokens)\n",
    "    \n",
    "    \n",
    "    tokens = tokens_re.findall(tokens)\n",
    "    \n",
    "    # Filter non stopwords, and the words starting with @\n",
    "    tokens = [item for item in tokens if item.lower() not in stopwords_list]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        # If it's emoticon then do not return lowercase\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "# tweet = 'RT @marcobonzanini: this is just an example! :D http://example.com #NLP'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2017, 1, 1, 0, 33, 12, tzinfo=tzfile('/usr/share/zoneinfo/EST'))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(data)):\n",
    "    data[j]['tokenized_tweet'] = preprocess(data[j]['text'], lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Next we need to learn and conduct sentiment analysis\n",
    "# TODO: Learn: https://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'jk', 'h', 's', 'k', 'hw', 'iuh', 'oi']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a through f: [] represents a range\n",
    "# Is two consecutive characters then use two ranges [a-f][a-f]\n",
    "re.split(r'[a-f]', 'ajkfhdsakFhweiuhfoi', flags = re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjfjadklsf fdasklfdj'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \\d{1,5} : digits length 1 to 5\n",
    "# +: 1 or more\n",
    "# .* anything\n",
    "# \\w alphanumeric (either letter or number)\n",
    "re.sub(r'\\d{1,5}\\s\\w+\\s\\w+\\.', ' ', 'adjfjadklsf475 wahsington st.fdasklfdj', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
